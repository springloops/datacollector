<?xml version="1.0" encoding="UTF-8"?>
<!--ield
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_wfr_rnw_yq">
 <title>Reusable Tables of Information</title>
 <shortdesc/>
 <conbody>
  <p>
   <draft-comment author="Loretta">The following IgnoreControlChar-row is used in Configuring a -
    Directory, File Tail, Kafka Consumer, Kinesis Consumer, JSON Parser, Log
    Parser:]</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_mxl_xrm_js">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row id="IgnoreControlChars-row">
       <entry>Ignore Ctrl Characters <xref href="../Pipeline_Design/ControlCharacters.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
          id="image_xwx_xrm_js"/></xref></entry>
       <entry>Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">ORIGIN rows - <uicontrol>ProduceSingleRec</uicontrol> is used by
    Kafka Consumer and MapR Streams Consumer. <uicontrol>MaxBatchSize</uicontrol> and
     <uicontrol>BatchWaitTime</uicontrol> rows are used in Configuring Kafka Consumer, JMS Consumer.
    See if they should go anywhere else.</draft-comment>
   <draft-comment author="Loretta">Charsets are used by message and file origins. But Directory
    Charset is standalone. </draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_tft_4jk_dt">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry/>
       <entry/>
      </row>
      <row id="ProduceSingleRec">
       <entry>Produce Single Record</entry>
       <entry>For each partition, generates a single record for records that include multiple
        objects. <p>When not selected, the origin generates multiple records when a record includes
         multiple objects.</p></entry>
      </row>
      <row id="MaxBatchSize">
       <entry>Max Batch Size (records)</entry>
       <entry>Maximum number of records processed at one time. Honors values up to the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> maximum
        batch size. <p>Default is 1000. The <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> default
         is 1000.</p></entry>
      </row>
      <row id="BatchWaitTime">
       <entry>Batch Wait Time (ms) <xref href="../Origins/BatchSizeWaitTime.dita#concept_ypd_vgr_5q">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_mgp_2q3_br"
          placement="inline"/></xref></entry>
       <entry>Number of milliseconds to wait before sending a partial or empty batch. </entry>
      </row>
      <row id="MessagesCharset">
       <entry>Charset</entry>
       <entry>Character encoding of the messages to be processed.<p>Not used for all data
         formats.</p></entry>
      </row>
      <row id="Charset">
       <entry>Charset</entry>
       <entry>Character encoding of the files to be processed.<p>Not used for all data
        formats.</p></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following rows are used in the Data Collector Console -
    Overview, </draft-comment>
  </p>
  <simpletable>
   <strow id="Icon-Help">
    <stentry><image href="../Graphics/icon_OverCHelp.png" id="image_bkz_wk3_ts"/></stentry>
    <stentry>Help icon</stentry>
    <stentry>Provides context-sensitive help based on the information in the panel. </stentry>
   </strow>
  </simpletable>
  <p>
   <draft-comment author="Loretta">The following row is used in Configuring Hive
    Streaming</draft-comment>
  </p>
  <table frame="all" rowsep="1" colsep="1" id="table_ps1_hln_jt">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry/>
      <entry/>
     </row>
    </thead>
    <tbody>
     <row id="FIELD2ColumnMapping">
      <entry>Field to Column Mapping</entry>
      <entry>
       <p>Use to override the default field to column mappings. </p>
       <p>By default, fields are written to columns of the same name. </p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="alisontaylor">The following descriptions are used by the AWS destinations:
    Amazon S3, Kinesis Firehose, Kinesis Producer. And by writing aggregated statistics to Kinesis,
    in DPM chapter > Aggregated Statistics for Pipelines > Configuring a Pipeline to Aggregate
    Statistics</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_vnv_ncr_mv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry>Access Key ID </entry>
       <entry id="AWSDest_AccessKeyID">
        <p>AWS access key ID.</p>
        <p>Required when not using IAM roles with IAM instance profile credentials.</p>
       </entry>
      </row>
      <row>
       <entry>Secret Access Key</entry>
       <entry id="AWSDest_SecretAccessKey">
        <p>AWS secret access key. </p>
        <p>Required when not using IAM roles with IAM instance profile credentials. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="alisontaylor">The following descriptions are used by the AWS origins:
    Amazon S3 and Kinesis Consumer.</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_lnx_51w_mv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry>Access Key ID</entry>
       <entry id="AWSOrigin_AccessKeyID">
        <p>AWS access key ID.</p>
        <p>Required when not using IAM roles with IAM instance profile credentials.</p>
       </entry>
      </row>
      <row>
       <entry>Secret Access Key</entry>
       <entry id="AWSOrigin_SecretAccessKey">
        <p>AWS secret access key. </p>
        <p>Required when not using IAM roles with IAM instance profile credentials. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="Loretta">The following rows are used in full for the Amazon S3 origin and
    the Amazon S3 destination reuses individual rows. Data Format and File Compression are also used
    in Directory and SFTP Client.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="AmazonS3-oProps">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>File Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="S3-Region">
       <entry>Region</entry>
       <entry>Amazon S3 region. </entry>
      </row>
      <row id="S3Bucket">
       <entry>Bucket</entry>
       <entry>Bucket that contains the objects to be read.</entry>
      </row>
      <row id="S3Folder">
       <entry>Common Prefix <xref href="../Origins/AmazonS3-CommonPrefixPatterns.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wm3_212_wv"/>
        </xref></entry>
       <entry>Optional common prefix that describes the location of the objects. When defined, the
        common prefix acts as a root for the prefix pattern.</entry>
      </row>
      <row id="S3ObjectPathDelimiter">
       <entry>Delimiter</entry>
       <entry>Delimiter used by Amazon S3 to define the prefix hierarchy.<p>Default is slash ( /
         ).</p></entry>
      </row>
      <row id="S3IncludeMetadata-row">
       <entry>Include Metadata <xref href="../Origins/AmazonS3-RecordHeaderAttrs.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_sw3_wjx_yw"/>
        </xref></entry>
       <entry>Includes system-defined and user-defined metadata in record header attributes.
       </entry>
      </row>
      <row id="S3FileNamePattern">
       <entry>Prefix Pattern <xref href="../Origins/AmazonS3-CommonPrefixPatterns.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_tfr_bk5_ht"/>
        </xref></entry>
       <entry>
        Prefix pattern that describes the objects to be processed.
        <p>You can include the entire path to the objects. You can also use Ant-style path patterns
         to read objects recursively. </p>
       </entry>
      </row>
      <row id="S3BufferLimit">
       <entry>Buffer Limit (KB)</entry>
       <entry>Maximum buffer size. The buffer size determines the size of the record that can be
        processed. <p>Decrease when memory on the <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> machine
         is limited. Increase to process larger records when memory is available. </p><p>Default is
         128 KB.</p></entry>
      </row>
      <row id="Origin-FileCompression">
       <entry>Compression Format <xref href="../Origins/FileCompressionFormats.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
          id="image_xqq_yv4_c5"/></xref></entry>
       <entry>The compression format of the files:<ul id="ul_vph_jp2_qs">
         <li>None - Processes only uncompressed files.</li>
         <li>Compressed File - Processes files compressed by the supported compression formats.</li>
         <li>Archive - Processes files archived by the supported archive formats.</li>
         <li>Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>
        </ul></entry>
      </row>
      <row id="S3DataFormat">
       <entry>Data Format <xref href="../Origins/AmazonS3-DataFormat.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_w4w_q3p_ht"/>
        </xref></entry>
       <entry id="entryDataFormats">Data format for source files. Use one of the following formats:<ul id="ul_y1t_wql_5q">
         <li>Avro</li>
         <li>Delimited</li>
         <li>JSON</li>
         <li>Log</li>
         <li>Protobuf</li>
         <li>SDC Record <xref href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
           <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wjh_ycl_br"
            placement="inline"/></xref></li>
         <li>Text</li>
         <li>Whole File <xref href="../Pipeline_Design/WholeFile.dita">
           <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
            id="image_igx_zzm_zw"/></xref></li>
         <li>XML</li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor">Kinesis Producer and DPM chapter > Aggregated Statistics for
   Pipelines > Configuring a Pipeline to Aggregate Statistics use these rows</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_kqy_mw1_fx">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <tbody>
     <row id="rowKinesisRegion">
      <entry>Region</entry>
      <entry>Amazon Web Services region that hosts the Kinesis cluster.</entry>
     </row>
     <row id="rowKinesisStreamName">
      <entry>Stream Name</entry>
      <entry>Kinesis stream name.</entry>
     </row>
     <row id="rowKinesisPartitionStrategy">
      <entry>Partitioning Strategy</entry>
      <entry>Strategy to write data to Kinesis shards:<ul id="ul_nqh_qw1_fx">
        <li>Random - Generates a random partition key.</li>
        <li>
         <p>Expression - Uses the result of an expression as the partition key.</p>
        </li>
       </ul></entry>
     </row>
     <row id="rowKinesisPartitionExp">
      <entry>Partition Expression</entry>
      <entry>Expression to generate the partition key used to pass data to different shards. <p>Use
        for the expression partition strategy. </p></entry>
     </row>
     <row id="rowKinesisConfig">
      <entry>Kinesis Producer Configuration</entry>
      <entry>Additional Kinesis properties. <p>When you add a configuration property, enter the
        exact property name and the value. The Kinesis Producer does not validate the property names
        or values. </p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">HTTP Client origin and processor rows:</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_jf4_g24_jw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="HTTP-ReqTransferEncod">
       <entry>Request Transfer Encoding</entry>
       <entry>Use one of the following encoding types:<ul id="ul_o3l_mgv_vw">
         <li>Buffered - The standard transfer encoding type. </li>
         <li>Chunked - Transfers data in chunks. Not supported by all servers.</li>
        </ul><p>The default is Chunked.</p></entry>
      </row>
      <row id="HTTP-ConnectTimeout">
       <entry>Connect Timeout</entry>
       <entry>Maximum number of milliseconds to wait for a connection. <p>Use 0 to wait
         indefinitely.</p></entry>
      </row>
      <row id="HTTP-ReadTimeout">
       <entry>Read Timeout</entry>
       <entry>Maximum number of milliseconds to wait for data. <p>Use 0 to wait
        indefinitely.</p></entry>
      </row>
      <row id="HTTP-AuthType">
       <entry>Authentication Type</entry>
       <entry>Determines the authentication type used to connect to the server:<ul
         id="ul_icm_h1l_35">
         <li>None - Performs no authentication.</li>
         <li>Basic - Uses basic authentication. Requires a username and password. <p>Use with HTTPS
           to avoid passing unencrypted credentials.</p></li>
         <li>Digest - Uses digest authentication. Requires a username and password.</li>
         <li>Universal - Makes an anonymous connection, then provides authentication credentials
          upon receiving a 401 status and a WWW-Authenticate header request. <p>Requires a username
           and password associated with basic or digest authentication.</p><p>Use only with servers
           that respond to this workflow.</p></li>
         <li>OAuth - Uses OAuth 1.0 authentication. Requires OAuth credentials.</li>
        </ul></entry>
      </row>
      <row id="HTTP-UseProxy">
       <entry>Use Proxy</entry>
       <entry>
        <p>Enables using an HTTP proxy to connect to the system. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">PROCESSOR rows</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_u2r_4x5_lv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.0*"/>
     <colspec colname="c2" colnum="2" colwidth="1.0*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="P-HashType">
       <entry>Hash Type</entry>
       <entry>Algorithm to use to hash field values:<ul id="ul_kmd_rnk_wq">
         <li>MD5 - Produces a 128-bit (16-byte) hash value, typically expressed in text format as a
          32 digit hexadecimal number.</li>
         <li>SHA1 - Produces a 160-bit (20-byte) hash value.</li>
         <li>SHA2 - Based on SHA1, but uses a set of four hash functions: 224, 256, 384, or 512
          bits.</li>
         <li>MURMUR3_128 - Produces a 128-bit (16 byte) hash value.</li>
        </ul></entry>
      </row>
      <row id="P-HashTargetField">
       <entry>Target Field</entry>
       <entry>Field in the record to use for hashed data. If the field does not exist, Field Hasher
        creates the field. </entry>
      </row>
      <row id="P-HashHeaderAtt">
       <entry>Header Attribute</entry>
       <entry>Attribute in the record header to use for hashed data. If the attribute does not
        exist, Field Hasher creates the attribute.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">The Hive Metadata processor and the Hive Metastore destination
   configuring topics use these individual rows and the Max entries description:</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_w1l_34y_dw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Hive Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="P-D_HiveJDBCURL">
       <entry>JDBC URL</entry>
       <entry>JDBC URL for Hive. You can use the default, or replace the expression for the database
        name with a specific database name when appropriate.</entry>
      </row>
      <row id="P-D_HiveJDBCdriver">
       <entry>JDBC Driver Name</entry>
       <entry>The fully-qualified JDBC driver name.</entry>
      </row>
      <row id="P-D_HiveConfigDir">
       <entry>Hadoop Configuration Directory</entry>
       <entry>
        <p>Absolute path to the directory containing the Hive and Hadoop configuration files. For a
         Cloudera Manager installation, enter hive-conf. </p>
        <p>The stage uses the following configuration files: <ul
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HiveStreamingFiles"
          id="ul_tqf_lms_dw">
          <li/>
         </ul></p>
        <note>Properties in the configuration files are overridden by individual properties defined
         in this processor. </note>
       </entry>
      </row>
      <row id="P-D-Hive-AddConfig">
       <entry>Additional Hadoop Configuration</entry>
       <entry>
        <p>Additional properties to use. </p>
        <p>To add properties, click <uicontrol>Add</uicontrol> and define the property name and
         value. Use the property names and values as expected by HDFS and Hive.</p>
       </entry>
      </row>
      <row>
       <entry>Max Cache Size (not conrefed)</entry>
       <entry id="P-D_HiveMaxCacheSize">Maximum number of entries in the cache. <p>When the cache
         reaches the maximum size, the oldest cached entries are evicted to allow for new
         data.</p><p>Default is -1, an unlimited cache size.</p></entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">DESTINATION rows</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_l3k_ksh_r5">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="D-CHARSET-file">
       <entry>Charset</entry>
       <entry>Character set to use when writing files. <p>Not used with all data
        formats.</p></entry>
      </row>
      <row id="D-CHARSET-other">
       <entry>Charset</entry>
       <entry>Character set to use when writing data. <p>Not used with all data formats.</p></entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="alisontaylor"><b>table-JDBCProps</b> - JDBC Consumer, Lookup, Producer,
    and Tee use most rows in this table. Oracle CDC is using a couple too. </draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_vwl_qcq_tw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.0*"/>
     <thead>
      <row>
       <entry>JDBC Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="JDBCConnectString_row">
       <entry>JDBC Connection String</entry>
       <entry>Connection string to use to connect to the database.<p>Some databases, such as
         Postgres, require the schema in the connection string. Use the connection string format
         required by the database.</p></entry>
      </row>
      <row id="FieldToColumnJDBC_row">
       <entry>Field to Column Mapping</entry>
       <entry>Use to override the default field to column mappings. By default, fields are written
        to columns of the same name.<p>When you override the mappings, you can define parameterized
         values to apply SQL functions to the field values before writing them to columns. For
         example, to convert a field value to an integer, enter the following for the parameterized
         value:<codeblock>CAST(? AS INTEGER)</codeblock></p><p>The question mark (?) is substituted
         with the value of the field. Leave the default value of ? if you do not need to apply a SQL
         function.</p><p>Use the <uicontrol>Add</uicontrol> icon to create additional field to
         column mappings.</p></entry>
      </row>
      <row id="JDBCUseCredentials_row">
       <entry>Use Credentials</entry>
       <entry>Enables entering credentials on the Credentials tab. Use when you do not include
        credentials in the JDBC connection string.</entry>
      </row>
      <row id="JDBCMaxClob_row">
       <entry>Max Clob Size (characters)</entry>
       <entry>Maximum number of characters to be read in a Clob field. Larger data is
        truncated.</entry>
      </row>
      <row id="LogFormat_row">
       <entry>Change Log Format</entry>
       <entry>Format of the change capture log. Use to specify the log format of origin data when
        processing change capture data.</entry>
      </row>
      <row id="RollbackBatch_row">
       <entry>Rollback Batch On Error</entry>
       <entry>Rolls back the entire batch when an error occurs within the batch. </entry>
      </row>
      <row id="MultiRowInsert_row">
       <entry>Use Multi-Row Insert</entry>
       <entry>Determines how the stage inserts records. Select to allow inserts of multiple rows at
        a time. Clear to insert a single row at a time. </entry>
      </row>
      <row id="ParamLimit_row">
       <entry>Statement Parameter Limit</entry>
       <entry>Defines the number of parameters allowed in the prepared statement for multi-row
         inserts.<p>Use -1 to disable the parameter limit. Default is -1.</p></entry>
      </row>
      <row id="JDBCAddtitionalProps_row">
       <entry>Additional JDBC Configuration Properties</entry>
       <entry>Additional JDBC configuration properties to use. To add properties, click
         <uicontrol>Add</uicontrol> and define the JDBC property name and value. <p>Use the property
         names and values as expected by JDBC. </p></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="Loretta"><b>table-JDBCAdvProps</b> - JDBC Consumer, JDBC Lookup, and JDBC
    Tee use the whole table, row by row. Producer uses all rows but last. Oracle CDC uses the whole
    table. </draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table-JDBCAdvProps">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Advanced Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="row-MaxPoolSize">
       <entry>Maximum Pool Size </entry>
       <entry>The maximum number of connections to create. <p>Default is 1. The recommended value is
         1.</p></entry>
      </row>
      <row id="row-MaxIdleConn">
       <entry>Minimum Idle Connections</entry>
       <entry>The minimum number of connections to create and maintain. To define a fixed connection
        pool, set to the same value as Maximum Pool Size. <p>Default is 1. </p></entry>
      </row>
      <row id="row-ConTimeout">
       <entry>Connection Timeout</entry>
       <entry>Maximum time to wait for a connection. Use a time constant in an expression to define
        the time increment. <p>Default is 30 seconds, defined as follows:
         <codeblock>${30 * SECONDS}</codeblock></p></entry>
      </row>
      <row id="row-IdleTimeout">
       <entry>Idle Timeout</entry>
       <entry>Maximum time to allow a connection to idle. Use a time constant in an expression to
        define the time increment. <p>Use 0 to avoid removing any idle connections.</p><p>Default is
         30 minutes, defined as follows: <codeblock>${30 * MINUTES}</codeblock></p></entry>
      </row>
      <row id="row-MaxConLife">
       <entry>Max Connection Lifetime</entry>
       <entry>Maximum lifetime for a connection. Use a time constant in an expression to define the
        time increment. <p>Use 0 to avoid removing any idle connections.</p><p>Default is 30
         seconds, defined as follows: <codeblock>${30 * SECONDS}</codeblock></p></entry>
      </row>
      <row id="row-EnReadOnly">
       <entry>Enforce Read-only Connection</entry>
       <entry>Creates read-only connections to avoid any type of write. <p>Selected by default.
         Clearing this property is not recommended. </p></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor"><b>table-S3AdvProps</b> Amazon S3 destination uses the whole
   table, S3 and Kinesis Consumer use certain rows.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table-S3AdvProps">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Advanced Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-UseProxy">
      <entry>Use Proxy</entry>
      <entry>Specifies whether to use a proxy to connect to Amazon S3.</entry>
     </row>
     <row id="row-ProxyHost">
      <entry>Proxy Host</entry>
      <entry>Proxy host.</entry>
     </row>
     <row id="row-ProxyPort">
      <entry>Proxy Port</entry>
      <entry>Proxy port.</entry>
     </row>
     <row id="row-ProxyUser">
      <entry>Proxy User</entry>
      <entry>User name for proxy credentials.</entry>
     </row>
     <row id="row-ProxyPassword">
      <entry>Proxy Password</entry>
      <entry>Password for proxy credentials. </entry>
     </row>
     <row>
      <entry>Thread Pool Size for Parallel Uploads</entry>
      <entry>Size of the thread pool for parallel uploads. Used when writing to multiple partitions
       and writing large objects in multiple parts.<p>When writing to multiple partitions, setting
        this property up to the number of partitions being written to can improve performance.
        </p><p>For more information about this and the following properties, see the Amazon S3
        TransferManager documentation.</p></entry>
     </row>
     <row>
      <entry>Multipart Upload Threshold</entry>
      <entry>Minimum batch size in bytes for the destination to use multipart uploads.</entry>
     </row>
     <row>
      <entry>Minimum Upload Part Size</entry>
      <entry>Minimum part size in bytes for multipart uploads.</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">HBase, Redis, and Static Lookup processors use rows in the
   table.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_ns1_p1s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Lookup Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-Mode">
      <entry>Mode</entry>
      <entry>Mode used to perform the lookups:<ul id="ul_t11_3fs_zv">
       <li>Per Batch - Performs a bulk lookup of all keys in a
        batch. The processor performs a single lookup for
        each batch.</li>
       <li>Per Key in Each Record - Performs individual lookups
        of each key in each record. If you configure
        multiple key expressions, the processor performs
        multiple lookups for each record.</li>
      </ul><p>Default is Per Batch.</p></entry>
     </row>
     <row>
      <entry>Enable Local Caching </entry>
      <entry id="entry-LocalCaching">Specifies whether to locally cache the returned key-value
       pairs.</entry>
     </row>
     <row id="row-MaxEntriesCache">
      <entry>Maximum Entries to Cache</entry>
      <entry>Maximum number of key-value pairs to cache. When the
       maximum number is reached, the processor evicts the oldest
       key-value pairs from the cache. <p>Default is -1, which
        means unlimited.</p></entry>
     </row>
     <row id="row-EvictionPolicy">
      <entry>Eviction Policy Type</entry>
      <entry>Policy used to evict key-value pairs from the local cache
       when the expiration time has passed:<ul id="ul_jql_yns_zv">
        <li>Expire After Last Access - Measures the expiration
         time since the key-value pair was last accessed by a
         read or a write.</li>
        <li>Expire After Last Write - Measures the expiration
         time since the key-value pair was created, or since
         the value was last replaced.</li>
       </ul></entry>
     </row>
     <row id="row-ExpirationTime">
      <entry>Expiration Time</entry>
      <entry>Amount of time that a key-value pair can remain in the
       local cache without being accessed or written to. <p>Default
        is 1 second.</p></entry>
     </row>
     <row id="row-timeUnit">
      <entry>Time Unit</entry>
      <entry>Unit of time for the expiration time. <p>Default is
       seconds.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">HBase destination and HBase Lookup use rows and entries in
   this table</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_bgt_kly_bw">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>HBase Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-ZooKeeperQuorum">
      <entry>ZooKeeper Quorum</entry>
      <entry>Comma-separated list of servers in the ZooKeeper quorum. Use the following format:
        <codeblock>&lt;host>.&lt;domain>.com</codeblock><p>To ensure a connection, enter additional
        broker URIs.</p></entry>
     </row>
     <row id="row-ZooKeeperClient">
      <entry>ZooKeeper Client Port</entry>
      <entry>Port number clients use to connect to the ZooKeeper servers. </entry>
     </row>
     <row id="row-ZooKeeperParent">
      <entry>ZooKeeper Parent Znode</entry>
      <entry>Root node that contains all znodes used by the HBase cluster.</entry>
     </row>
     <row id="row-TableName">
      <entry>Table Name</entry>
      <entry>Name of the HBase table to use. Enter a table name or a namespace and table name as
       follows: &lt;namespace>.&lt;tablename>. <p>If you do not enter a table name, HBase uses the
        default namespace. </p></entry>
     </row>
     <row>
      <entry>Kerberos Authentication </entry>
      <entry id="entry-Kerberos">Uses Kerberos credentials to connect to HBase.<p>When selected,
        uses the Kerberos principal and keytab defined in the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
        configuration file, <codeph>$SDC_CONF/sdc.properties</codeph>. </p></entry>
     </row>
     <row>
      <entry>HBase User </entry>
      <entry id="entry-HBaseUser">The HBase user to use to connect to HBase. When using this
       property, make sure HBase is configured appropriately.<p>By default, the pipeline uses the
         <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> user
        to connect to HBase.</p></entry>
     </row>
     <row>
      <entry>HBase Configuration Directory</entry>
      <entry id="entry-HBaseConfigDirectory">Location of the HDFS configuration files. <p>For a
        Cloudera Manager installation, enter <codeph>hbase-conf</codeph>. For all other
        installations, use a directory or symlink within the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> resources
        directory.</p><p>You can use the following file with HBase:<ul
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HDFSfiles_HBasedest"
         id="ul_ezj_cvr_bt">
         <li/>
        </ul></p><note>Properties in the configuration files are overridden by individual properties
        defined in the stage.</note></entry>
     </row>
     <row>
      <entry>HBase Configuration</entry>
      <entry id="entry-HBaseConfig">
       <p>Additional HBase configuration properties to use. </p>
       <p>To add properties, click <uicontrol>Add</uicontrol> and
        define the property name and value. Use the property
        names and values as expected by HBase. </p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Redis origin, destination, and Lookup processor use this
   row</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_i5s_54s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Redis Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-RedisURI">
      <entry>URI</entry>
      <entry>URI of the Redis server. Use the following
        format:<codeblock>redis://&lt;host name>:&lt;port number>/&lt;database></codeblock><p>You
        can omit the port number or database if the server uses the default port number or default
        database.</p><p>You can optionally include your password to log in to the Redis server. For
        example:<codeblock>redis://:&lt;password>@&lt;host name>:&lt;port number>/&lt;database></codeblock></p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">MongoDB origin and destination use all rows in this table.
   They use the description for the Enable SSL property</draft-comment>
    <table frame="all" rowsep="1" colsep="1" id="table_xwz_jsd_3t">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1.5*"/>
      <colspec colname="c2" colnum="2" colwidth="3.5*"/>
      <thead>
       <row>
        <entry>Advanced Property</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row id="MongoDB_Connections_row">
      <entry>Connections Per Host</entry>
      <entry>Maximum number of connections for each host.<p>Default is 100.</p></entry>
     </row>
       <row id="MongoDB_MinConnections_row">
      <entry>Min Connections Per Host</entry>
      <entry>Minimum number of connections for each host.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_ConnectionTimeout_row">
      <entry>Connection Timeout</entry>
      <entry>Maximum time in milliseconds to wait for a connection. <p>Default is
       10,000.</p></entry>
     </row>
       <row id="MongoDB_MaxConnIdleTime_row">
      <entry>Max Connection Idle Time</entry>
      <entry>Maximum time in milliseconds that a pooled connection can remain idle. When a pooled
       connection exceeds the idle time, the connection is closed. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_MaxConnLifetime_row">
      <entry>Max Connection Lifetime</entry>
      <entry>Maximum time in milliseconds that a pooled connection can be active. When a pooled
       connection exceeds the lifetime, the connection is closed. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_MaxWaitTime_row">
      <entry>Max Wait Time</entry>
      <entry>Maximum time in milliseconds that a thread can wait for a connection to become
       available. Use 0 to opt out of this property. Use a negative value to wait
        indefinitely.<p>Default is 120,000.</p></entry>
     </row>
       <row id="MongoDB_ServerTimeout_row">
      <entry>Server Selection Timeout</entry>
      <entry>Maximum time in milliseconds that <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> waits for a server selection
       before throwing an exception. If you use 0, an exception is thrown immediately if no server
       is available. Use a negative value to wait indefinitely.<p>Default is 30,000.</p></entry>
     </row>
       <row id="MongoDB_ThreadsAllowed_row">
      <entry>Threads Allowed to Block for Connection Multiplier</entry>
      <entry>Multiplier that determines the maximum number of threads that can wait for a connection
       to become available from the pool. This number multiplied by the Connections Per Host value
       determines the maximum number of threads.<p>Default is 5.</p></entry>
     </row>
       <row id="MongoDB_Heartbeat_row">
      <entry>Heartbeat Frequency</entry>
      <entry>The frequency in milliseconds at which <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> attempts to determine the
       current state of each server in the cluster.<p>Default is 10,000.</p></entry>
     </row>
       <row id="MongoDB_MinHeartbeat_row">
      <entry>Min Heartbeat Frequency</entry>
      <entry>Minimum heartbeat frequency in milliseconds. <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> waits at least this long
       before checking the state of each server.<p>Default is 500.</p></entry>
     </row>
       <row id="MongoDB_HeartbeatConn_row">
      <entry>Heartbeat Connection Timeout</entry>
      <entry>Maximum time in milliseconds to wait for a connection used for the cluster
        heartbeat.<p>Default is 20,000.</p></entry>
     </row>
       <row id="MongoDB_HeartbeatSocket_row">
      <entry>Heartbeat Socket Timeout</entry>
      <entry>Maximum time in milliseconds for a socket timeout for connections used for the cluster
        heartbeat.<p>Default is 20,000.</p></entry>
     </row>
       <row id="MongoDB_LocalThreshold_row">
      <entry>Local Threshold</entry>
      <entry>Local threshold in milliseconds. Requests are sent to a server whose ping time is less
       than or equal to the server with the fastest ping time plus the local threshold
        value.<p>Default is 15.</p></entry>
     </row>
       <row id="MongoDB_Replica_row">
      <entry>Required Replica Set Name</entry>
      <entry>Required replica set name to use for the cluster.</entry>
     </row>
       <row id="MongoDB_Cursor_row">
      <entry>Cursor Finalizer Enabled</entry>
      <entry>Specifies whether to enable cursor finalizers.</entry>
     </row>
       <row id="MongoDB_SocketKeepAlive_row">
      <entry>Socket Keep Alive</entry>
      <entry>Specifies whether to enable socket keep alive. </entry>
     </row>
       <row id="MongoDB_SocketTimeout_row">
      <entry>Socket Timeout</entry>
      <entry>Maximum time in milliseconds for the socket timeout. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_SSLInvalid_row">
      <entry>SSL Invalid Host Name Allowed</entry>
      <entry>Specifies whether invalid host names are allowed in SSL certificates.</entry>
     </row>
      </tbody>
     </tgroup>
    </table>
 </conbody>
</concept>
