<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_y5w_dj3_fw">
 <title>Implementation Steps</title>
 <conbody>
  <p><indexterm>Hive Drift Solution<indexterm>implementation</indexterm></indexterm>To
            implement the Hive Drift Solution, perform the following steps:<ol
                id="ol_zr1_3j3_fw">
                <li> Configure the origin and any additional processors that you want to use. <ul
                        id="ul_qjn_kp4_hx">
                        <li>If using the JDBC Consumer as the origin, enable the creation of JDBC
                            header attributes. For more information, see <xref
                                href="../Origins/JDBCConsumer-HeaderHDS.dita#concept_tvf_tgp_fx"/>. </li>
                        <li>If data includes records with nested fields, add a Field Flattener to
                            flatten records before passing them to the Hive Metadata processor.</li>
                    </ul></li>
                <li>To capture columnar drift and to enable record-based writes, configure the Hive
                    Metadata processor:<ul id="ul_qsk_5x3_fw">
                        <li>Configure the Hive connection information.</li>
                        <li>Configure the database, table, and partition expressions.<p>You can
                                enter a single name or use an expression that evaluates to the names
                                to use. If necessary, you can use an Expression Evaluator earlier in
                                the pipeline to write the information to a record field or record
                                header attribute.</p></li>
                        <li>Configure the decimal field precision and scale expressions.<p>You can
                                use constants or expressions that evaluate to the same precision and
                                scale for all decimal fields. Or, you can create more complex
                                expressions that evaluate to different values for different fields.
                                </p><p>When processing data from the JDBC Consumer with JDBC header
                                attributes, use the default expressions. </p></li>
                        <li>Optionally configure advanced options, such as the maximum cache size,
                            time basis, and data time zone.</li>
                    </ul><p>For more information about the Hive Metadata processor, see <xref
                            href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv"/>.</p></li>
                <li>To process metadata records generated by the processor and alter Hive tables as
                    needed, connect the metadata output of the Hive Metadata processor to the Hive
                    Metastore destination.<p>
                        <note>While you might filter or route some records away from the Hive
                            Metastore destination, the destination must receive metadata records to
                            update Hive tables.</note>
                    </p></li>
                <li>Configure the Hive Metastore destination:<ul id="ul_k12_ty3_fw">
                        <li>Configure the Hive connection information.</li>
                        <li>Optionally configure cache information and how tables are updated.</li>
                    </ul><p>For more information about the Hive Metastore destination, see <xref
                            href="../Destinations/HiveMetastore.dita#concept_gcr_z2t_zv"/>.</p></li>
                <li>Connect the data output of the Hive Metadata processor to the Hadoop FS or MapR
                    FS destination to write records to the destination system using record header
                    attributes.</li>
                <li>Configure the Hadoop FS or MapR FS destination:<ul id="ul_n4p_dz3_fw">
                        <li>To write records using the targetDirectory header attribute, on the
                                <wintitle>Output Files</wintitle> tab, select <uicontrol>Directory
                                in Header</uicontrol>.</li>
                        <li>To roll records based on a roll header attribute, on the
                                <wintitle>Output Files</wintitle> tab, select <uicontrol>Use Roll
                                Attribute</uicontrol>, and for <uicontrol>Roll Attribute
                                Name</uicontrol>, enter “roll”.</li>
                        <li>To write records using the avroSchema header attribute, on the
                                <wintitle>Avro</wintitle> tab, select <uicontrol>Load Schema From
                                Header</uicontrol>.</li>
                    </ul><p>For more information about using record header attributes, see <xref
                            href="../Destinations/RecordHeaderAttributes.dita#concept_lmn_gdc_1w"
                        />.</p></li>
            </ol></p>
 </conbody>
</concept>
