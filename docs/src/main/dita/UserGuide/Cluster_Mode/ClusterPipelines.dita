<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_hmh_kfn_1s">
 <title>Cluster Pipelines</title>
 <shortdesc>A <term>cluster pipeline</term> is a pipeline that runs in cluster execution mode. You
  can run a pipeline in standalone execution mode or cluster execution mode. </shortdesc>
 <conbody>
  <p><indexterm>execution mode<indexterm>standalone and cluster
    modes</indexterm></indexterm><indexterm>cluster
    mode<indexterm>description</indexterm></indexterm><indexterm>standalone
     mode<indexterm>description</indexterm></indexterm>In standalone mode, a single <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> process runs
   the pipeline. A pipeline runs in standalone mode by default. </p>
  <p>In cluster mode, the <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> uses a cluster
   manager and a cluster application to spawn additional workers as needed. Use cluster mode to read
   data from a Kafka cluster, MapR cluster, or HDFS.</p>
  <p>When would you choose standalone or cluster mode? Say you want to ingest logs from applications
   servers and perform a computationally expensive transformation. To do this, you might use a set
   of standalone pipelines to stream log data from each application server to a Kafka or MapR
   cluster. And then use a cluster pipeline to process the data from the cluster and perform the
   expensive transformation.</p>
  <p>Or, you might use cluster mode to move data from HDFS to another destination, such as
   Elasticsearch.</p>
  <p>The origin system determines how a <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> runs a cluster
   mode pipeline:<dl>
    <dlentry>
     <dt>Kafka cluster</dt>
     <dd><ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> can
      process data from a Kafka cluster in cluster streaming execution mode. In cluster streaming
      mode, the <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
      /> processes data continuously until you stop the pipeline. </dd>
     <dd>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
      runs as an application within Spark Streaming, an open source cluster-computing application.
      Spark Streaming runs on either the Mesos or YARN cluster manager to process data from a Kafka
      cluster.</dd>
     <dd>The cluster manager and Spark Streaming spawn a <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> worker for
      each topic partition in the Kafka cluster. So each partition has a <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> worker to
      process data. </dd>
     <dd>Use the Kafka Consumer origin to process data from a Kafka cluster.</dd>
    </dlentry>
   </dl><dl>
    <dlentry>
     <dt>MapR cluster</dt>
     <dd><ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> can
      process data from a MapR cluster in cluster streaming execution mode. In cluster streaming
      mode, the <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
      /> processes data continuously until you stop the pipeline. </dd>
     <dd>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
      runs as an application within Spark Streaming, an open source cluster-computing application.
      Spark Streaming runs on a YARN cluster manager to process data from a MapR cluster.</dd>
     <dd>The cluster manager and Spark Streaming spawn a <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> worker for
      each topic partition in the MapR cluster. So each partition has a <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> worker to
      process data. </dd>
     <dd>Use the MapR Streams Consumer origin to process data from a MapR cluster.</dd>
    </dlentry>
    <dlentry>
     <dt>HDFS</dt>
     <dd><ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> can
      process data from HDFS in cluster batch mode: In cluster batch mode, the <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> processes
      all available data and then stops the pipeline. </dd>
     <dd>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
      runs as an application on top of MapReduce, an open-source cluster-computing framework.
      MapReduce runs on a YARN cluster manager. </dd>
     <dd>YARN and MapReduce generate additional worker nodes as needed. MapReduce creates one map
      task for each HDFS block. </dd>
     <dd>Use the Hadoop FS origin to process data from HDFS in cluster batch mode.</dd>
    </dlentry>
   </dl></p>
  <p>You can use any processor or destination in cluster pipelines.</p>
 </conbody>
</concept>
